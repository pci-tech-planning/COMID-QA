# ğŸ“Š README: 2\_analytical-engine

**Purpose:**
The `2_analytical-engine/` folder contains all scripts, reusable functions, and datasets used to process, transform, and analyze hydrologic and streamflow data for the COMID-QA project. This stage forms the core logic layer that connects raw data sources to reproducible outputs.

---

## ğŸ“ Folder Structure

```
2_analytical-engine/
â”œâ”€â”€ data/             # Processed inputs and outputs (e.g., matched COMIDs, daily flows)
â”œâ”€â”€ notebooks/        # Quarto notebooks for exploration, mapping, and QA
â”œâ”€â”€ scripts/          # Single-use scripts (e.g., USGS data downloader)
â”œâ”€â”€ source/           # Reusable functions (e.g., match_comid)
â”œâ”€â”€ tests/            # Lightweight test files to verify function behavior
â”œâ”€â”€ COMID-QA.Rproj    # Project-level config
â””â”€â”€ README.md         # This file
```

---

## ğŸ§  Key Components

### `source/match_comid.R`

Function to assign the nearest NHDPlus COMID to each input gaging station based on coordinates. Used for aligning observed and modeled streamflow data.

### `scripts/get_usgs_daily_flow_or.R`

Downloads daily discharge data for Oregonâ€™s HCDN stations using `dataRetrieval`. Can be modified to handle other regions.

### `tests/test_match_comid.R`

Tests `match_comid()` with a small gage sample and verifies COMID assignment.

---

## ğŸ”„ Workflow Overview

1. **Prepare** HCDN station list (filtered to Oregon)
2. **Match** to COMIDs using `match_comid()` and NHDPlusV2 shapefiles
3. **Download** daily flow using `dataRetrieval`
4. **Analyze or visualize** in notebooks

---

## ğŸ” See Also

* Raw data sources live in `1_knowledge-base/datasets/raw_data/`
* Final products and visualizations move to `3_production_hub/` or `4_product_store/`

---

ğŸ“Œ Maintainer: Francisco J. Guerrero
ğŸ› ï¸ Last updated: \[Insert date here]
